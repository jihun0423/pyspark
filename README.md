# pyspark

이전 프로젝트때 데이터 크기가 큰 데이터셋으로 정제 및 분석을 해본 적이 있었는데, 전처리할 때도 시간이 생각보다 소요됐고, 특히 머신러닝 모델을 통하여 학습할때 시간이 매우 걸려서 다양한 시도를 하지 못했었다.  
이때 이 문제를 해결하고자 방법을 찾던 중, spark를 이용하여 분산처리를 하면 속도가 향상된다는 것을 알게 되었지만, 그때 당시는 학습할 시간이 없어 설치만 해두었었다.  
앞으로 다양한 데이터들을 접하게 될텐데, 그 때를 대비하고자 pyspark를 학습하기로 하였다.


현재 듣고 있는 강의에서는 Databricks를 사용하여 실습을 하였지만, databricks는 무료 버전의 경우 들어갈 때마다 클러스터를 생성하여 실행해 주어야 하기 때문에, 시간이 걸려서 로컬에 pyspark를 설치하여 사용하기로 하였고, 성공적으로 설치하였다.  


* 05.14 : pyspark를 로컬에 설치 후 사용하던 중, sql function을 쓰던 중 오류가 발생하여 원인을 찾아보니 하둡의 경로를 환경변수에 추가하지 않아서 생긴 것이였다. 오류를 해결 한 후 sql function을 사용해 보았는데, 기존에 알고있던 sql과 굉장히 유사해서 처음엔 익숙하게 할 수 있을거라고 생각하였지만, 사용할 수록 약간의 차이점들이 쌓이고 쌓여서 속도가 매우 더뎌졌다. pyspark에서의 문법에 익숙해 지는게 중요할 것 같다. 
* 05.18 : 이제 pyspark에서의 문법에 대해서 학습은 마쳤지만 여전히 코드를 직접 짤때 사소하게 틀리는 경우가 많다. 특히, pandas dataframe에서는 행 삭제 같은 것이 매우 간편하게 drop을 통해 이루어 졌다면, pyspark dataframe에서는 행 삭제 기능은 일체 없고, 오히려 필터링을 통하여 삭제하고 싶은 행을 걸러내는 느낌으로 사용해야 한다는 것 때문에 평소 하던 습관처럼 코드를 짤 수가 없어 불편했다.
